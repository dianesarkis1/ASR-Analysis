{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc863297",
   "metadata": {},
   "source": [
    "# ASR Phonology Inference: Whisper v3 (French & Spanish)\n",
    "\n",
    "This notebook quantifies which **phonology/prosody features** drive **WER** and how much of the **Cartesia transform** effect can be explained by those features.\n",
    "\n",
    "**What it does** (for **French** and **Spanish** separately):\n",
    "1. **Assemble datasets**:\n",
    "   - *Line-paired deltas* (only lines present in both Original and Transformed)\n",
    "   - *Speaker-paired deltas* (means per speaker, then difference)\n",
    "   - *Pooled long set* (all observations with `treatment ∈ {0,1}`)\n",
    "2. **Descriptives** and sanity checks.\n",
    "3. **Δ‑Regression (primary inference)**: `Δ log1p(WER) ~ Σ βk ΔXk` with ridge CV → refit OLS for interpretable coefficients + clustered SEs (by speaker).\n",
    "4. **Mixed‑effects confirmation** on the pooled set with random intercepts for speaker/country/gender and `treatment × features` interactions.\n",
    "5. **Mediation**: estimate proportion of the transform’s effect explained via measured features (bootstrap CIs).\n",
    "6. **Chi‑square tests**:\n",
    "   - Improvement vs worsening counts (overall and by language)\n",
    "   - Improvement vs feature‑change quartiles\n",
    "7. **Robustness**: speaker‑paired deltas; winsorization/Huber; PCA variant.\n",
    "\n",
    "> ⚠️ Assumptions:  \n",
    "> • Your files are split into two top‑level folders: `original/` and `transformed/`, and further into `french/` and `spanish/`.  \n",
    "> • Each CSV file corresponds to a `(country, gender)` combination and contains columns:  \n",
    "> `speaker_id,line_id,rms_amplitude,dc_offset,articulation_rate,mean_pitch,pitch_std_dev,hnr,wer_score`.  \n",
    "> • Filenames follow a pattern like: `belgium_female_*.csv` (country and gender recoverable via regex).  \n",
    "> If your layout differs, edit the **Config** cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "from pathlib import Path\n",
    "LANGUAGES = [\"french\", \"spanish\"]          # analyze separately\n",
    "DATA_ROOT = Path(\"data\")                    # change to your dataset root (relative to notebook)\n",
    "ORIG_DIR = DATA_ROOT / \"original\"\n",
    "TRANS_DIR = DATA_ROOT / \"transformed\"\n",
    "\n",
    "# filename like \"belgium_female_individual_sample_features.csv\"\n",
    "# adjust the regex if your names differ\n",
    "FILENAME_RE = r\"(?P<country>[a-z_]+)_(?P<gender>male|female).+\\.csv\"\n",
    "\n",
    "FEATURES = [\"rms_amplitude\",\"dc_offset\",\"articulation_rate\",\"mean_pitch\",\"pitch_std_dev\",\"hnr\"]\n",
    "OUTCOME = \"wer_score\"\n",
    "ID_COLS = [\"speaker_id\",\"line_id\"]\n",
    "GROUP_COLS = [\"country\",\"gender\",\"language\"]\n",
    "TREATMENT_NAME = \"treatment\"               # 0=original, 1=transformed\n",
    "RANDOM_SEED = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d276736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import re, math, json, warnings, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "from statsmodels.robust.norms import HuberT\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6118fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper functions ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_meta_from_filename(path: Path, language: str):\n",
    "    m = re.search(FILENAME_RE, path.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Filename doesn't match expected pattern: {path.name}\")\n",
    "    return {\n",
    "        \"country\": m.group(\"country\"),\n",
    "        \"gender\": m.group(\"gender\"),\n",
    "        \"language\": language\n",
    "    }\n",
    "\n",
    "def load_language_tables(language: str, base_orig: Path, base_trans: Path):\n",
    "    # Load original & transformed CSVs for a given language\n",
    "    orig_dir = base_orig / language\n",
    "    trans_dir = base_trans / language\n",
    "    if not orig_dir.exists() or not trans_dir.exists():\n",
    "        raise FileNotFoundError(f\"Expected folders for language '{language}' not found at {orig_dir} or {trans_dir}\")\n",
    "    \n",
    "    def load_dir(d: Path, treatment_flag: int):\n",
    "        frames = []\n",
    "        for p in sorted(d.glob(\"*.csv\")):\n",
    "            meta = parse_meta_from_filename(p, language)\n",
    "            df = pd.read_csv(p)\n",
    "            # attach meta\n",
    "            for k,v in meta.items():\n",
    "                df[k] = v\n",
    "            df[\"treatment\"] = treatment_flag\n",
    "            frames.append(df)\n",
    "        if not frames:\n",
    "            raise FileNotFoundError(f\"No CSV files found in {d}\")\n",
    "        return pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    orig = load_dir(orig_dir, 0)\n",
    "    trans = load_dir(trans_dir, 1)\n",
    "    return orig, trans\n",
    "\n",
    "def zscore_within_language(df: pd.DataFrame, feature_cols):\n",
    "    # standardize features within each language on pooled (orig+trans) data\n",
    "    df = df.copy()\n",
    "    for lang, sub in df.groupby(\"language\"):\n",
    "        scaler = StandardScaler()\n",
    "        vals = scaler.fit_transform(sub[feature_cols])\n",
    "        df.loc[sub.index, [f\"{c}_z\" for c in feature_cols]] = vals\n",
    "    return df\n",
    "\n",
    "def make_line_paired_deltas(orig: pd.DataFrame, trans: pd.DataFrame, feature_cols):\n",
    "    # Inner-join on (country, gender, language, speaker_id, line_id)\n",
    "    idx_cols = [\"country\",\"gender\",\"language\",\"speaker_id\",\"line_id\"]\n",
    "    left = orig[idx_cols + feature_cols + [OUTCOME]].rename(columns={c:f\"{c}_orig\" for c in feature_cols+[OUTCOME]})\n",
    "    right = trans[idx_cols + feature_cols + [OUTCOME]].rename(columns={c:f\"{c}_trans\" for c in feature_cols+[OUTCOME]})\n",
    "    merged = pd.merge(left, right, on=idx_cols, how=\"inner\")\n",
    "    if merged.empty:\n",
    "        print(\"⚠️ No line-level pairs found after inner join. Check IDs/cleaning.\")\n",
    "    # deltas\n",
    "    for c in feature_cols:\n",
    "        merged[f\"d_{c}\"] = merged[f\"{c}_trans\"] - merged[f\"{c}_orig\"]\n",
    "    merged[\"d_wer\"] = merged[f\"{OUTCOME}_trans\"] - merged[f\"{OUTCOME}_orig\"]\n",
    "    # log1p WER\n",
    "    merged[\"d_log1p_wer\"] = np.log1p(merged[f\"{OUTCOME}_trans\"]) - np.log1p(merged[f\"{OUTCOME}_orig\"])\n",
    "    return merged\n",
    "\n",
    "def make_speaker_paired_deltas(orig: pd.DataFrame, trans: pd.DataFrame, feature_cols):\n",
    "    grp = [\"country\",\"gender\",\"language\",\"speaker_id\"]\n",
    "    agg = {c:\"mean\" for c in feature_cols+[OUTCOME]}\n",
    "    o = orig.groupby(grp, as_index=False).agg(agg).rename(columns={c:f\"{c}_orig\" for c in feature_cols+[OUTCOME]})\n",
    "    t = trans.groupby(grp, as_index=False).agg(agg).rename(columns={c:f\"{c}_trans\" for c in feature_cols+[OUTCOME]})\n",
    "    merged = pd.merge(o, t, on=grp, how=\"inner\")\n",
    "    # counts for weights\n",
    "    cnt_o = orig.groupby(grp)[\"line_id\"].count().rename(\"n_orig\")\n",
    "    cnt_t = trans.groupby(grp)[\"line_id\"].count().rename(\"n_trans\")\n",
    "    merged = merged.merge(cnt_o, on=grp).merge(cnt_t, on=grp)\n",
    "    merged[\"weight\"] = 2 * merged[\"n_orig\"] * merged[\"n_trans\"] / (merged[\"n_orig\"] + merged[\"n_trans\"]).replace(0, np.nan)\n",
    "    for c in feature_cols:\n",
    "        merged[f\"d_{c}\"] = merged[f\"{c}_trans\"] - merged[f\"{c}_orig\"]\n",
    "    merged[\"d_wer\"] = merged[f\"{OUTCOME}_trans\"] - merged[f\"{OUTCOME}_orig\"]\n",
    "    merged[\"d_log1p_wer\"] = np.log1p(merged[f\"{OUTCOME}_trans\"]) - np.log1p(merged[f\"{OUTCOME}_orig\"])\n",
    "    return merged\n",
    "\n",
    "def make_pooled_long(orig: pd.DataFrame, trans: pd.DataFrame):\n",
    "    pooled = pd.concat([orig, trans], ignore_index=True)\n",
    "    pooled[\"log1p_wer\"] = np.log1p(pooled[OUTCOME])\n",
    "    return pooled\n",
    "\n",
    "def bh_fdr(pvals, alpha=0.05):\n",
    "    rej, pcor, _, _ = multipletests(pvals, alpha=alpha, method=\"fdr_bh\")\n",
    "    return rej, pcor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4bff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data (edit paths if needed) ===\n",
    "all_results = {}\n",
    "for language in LANGUAGES:\n",
    "    print(f\"Loading language: {language}\")\n",
    "    orig, trans = load_language_tables(language, ORIG_DIR, TRANS_DIR)\n",
    "    # attach to dict\n",
    "    all_results[language] = {\n",
    "        \"orig\": orig,\n",
    "        \"trans\": trans,\n",
    "        \"line_paired\": make_line_paired_deltas(orig, trans, FEATURES),\n",
    "        \"speaker_paired\": make_speaker_paired_deltas(orig, trans, FEATURES),\n",
    "        \"pooled\": make_pooled_long(orig, trans),\n",
    "    }\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65577522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Descriptives & sanity checks ===\n",
    "for language, d in all_results.items():\n",
    "    print(f\"\\n--- {language.upper()} ---\")\n",
    "    lp = d[\"line_paired\"]\n",
    "    sp = d[\"speaker_paired\"]\n",
    "    pooled = d[\"pooled\"]\n",
    "    print(f\"Line-paired rows: {len(lp):,}\")\n",
    "    print(f\"Speaker-paired rows: {len(sp):,}\")\n",
    "    print(f\"Pooled rows: {len(pooled):,}\")\n",
    "    \n",
    "    # Basic summaries\n",
    "    print(pooled.groupby(\"treatment\")[OUTCOME].describe()[[\"mean\",\"std\",\"50%\",\"min\",\"max\"]])\n",
    "    \n",
    "    # Improvement counts (line-paired only)\n",
    "    if len(lp):\n",
    "        improved = (lp[\"d_wer\"] < 0).sum()\n",
    "        worsened = (lp[\"d_wer\"] >= 0).sum()\n",
    "        print(f\"Improved (ΔWER<0): {improved}, Worsened: {worsened}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d568b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Δ-Regression (primary inference) ===\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "results_delta = {}\n",
    "for language, d in all_results.items():\n",
    "    lp = d[\"line_paired\"].copy()\n",
    "    if lp.empty:\n",
    "        print(f\"{language}: no line-paired data; skipping Δ-regression.\")\n",
    "        continue\n",
    "    \n",
    "    # Standardize Δ features within language\n",
    "    dcols = [f\"d_{c}\" for c in FEATURES]\n",
    "    # Replace any inf/nan\n",
    "    lp = lp.replace([np.inf,-np.inf], np.nan).dropna(subset=dcols+[\"d_log1p_wer\",\"speaker_id\"])\n",
    "    \n",
    "    X = lp[dcols].values\n",
    "    y = lp[\"d_log1p_wer\"].values\n",
    "    speakers = lp[\"speaker_id\"].astype(str).values\n",
    "    \n",
    "    # Ridge CV with GroupKFold by speaker for λ selection\n",
    "    gkf = GroupKFold(n_splits=min(5, len(np.unique(speakers))))\n",
    "    alphas = np.logspace(-3, 3, 25)\n",
    "    ridge = RidgeCV(alphas=alphas, cv=gkf)\n",
    "    ridge.fit(X, y, groups=speakers)\n",
    "    \n",
    "    # Selected alpha and coefficients\n",
    "    coefs = pd.Series(ridge.coef_, index=dcols, name=\"ridge_coef\")\n",
    "    \n",
    "    # Refit OLS for interpretable β + clustered SEs (by speaker)\n",
    "    df_model = lp[dcols + [\"d_log1p_wer\",\"speaker_id\"]].copy()\n",
    "    formula = \"d_log1p_wer ~ \" + \" + \".join(dcols)\n",
    "    ols_fit = ols(formula, data=df_model).fit(cov_type=\"cluster\", cov_kwds={\"groups\": df_model[\"speaker_id\"]})\n",
    "    summ = pd.DataFrame({\n",
    "        \"beta_ols\": ols_fit.params.reindex([\"Intercept\"]+dcols, fill_value=np.nan),\n",
    "        \"se_clustered\": ols_fit.bse.reindex([\"Intercept\"]+dcols, fill_value=np.nan),\n",
    "        \"pval\": ols_fit.pvalues.reindex([\"Intercept\"]+dcols, fill_value=np.nan)\n",
    "    })\n",
    "    # FDR on feature p-values\n",
    "    mask = summ.index.isin(dcols)\n",
    "    rej, pcor = bh_fdr(summ.loc[mask,\"pval\"].values, alpha=0.05)\n",
    "    summ.loc[mask,\"pval_fdr\"] = pcor\n",
    "    summ.loc[mask,\"significant_fdr\"] = rej\n",
    "    \n",
    "    results_delta[language] = {\n",
    "        \"ridge_alpha\": ridge.alpha_,\n",
    "        \"ridge_coefs\": coefs,\n",
    "        \"ols_summary\": summ,\n",
    "        \"ols_model\": ols_fit\n",
    "    }\n",
    "    print(f\"\\\\n[{language.upper()}] Δ-Regression done. Selected ridge alpha = {ridge.alpha_:.4g}\")\n",
    "    display(summ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c91e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Chi-square tests ===\n",
    "from scipy.stats import chi2_contingency, binomtest\n",
    "\n",
    "def chisq_improvement(language, d):\n",
    "    lp = d[\"line_paired\"]\n",
    "    if lp.empty:\n",
    "        return None\n",
    "    tbl = pd.crosstab(index=[language], columns=(lp[\"d_wer\"]<0).map({True:\"Improved\", False:\"Worsened\"}))\n",
    "    return tbl\n",
    "\n",
    "# 1) Overall improved vs worsened per language + binomial test\n",
    "all_tbls = []\n",
    "for language, d in all_results.items():\n",
    "    lp = d[\"line_paired\"]\n",
    "    if lp.empty:\n",
    "        print(f\"{language}: no line-paired data, skipping chi-square.\")\n",
    "        continue\n",
    "    improved = int((lp[\"d_wer\"]<0).sum())\n",
    "    worsened = int((lp[\"d_wer\"]>=0).sum())\n",
    "    n = improved + worsened\n",
    "    print(f\"\\\\n[{language.upper()}] Improved={improved}, Worsened={worsened}\")\n",
    "    # Binomial test vs 50/50\n",
    "    bt = binomtest(improved, n, p=0.5, alternative=\"two-sided\")\n",
    "    print(f\"Binomial test p-value (improvement rate ≠ 50%): {bt.pvalue:.4g}\")\n",
    "    all_tbls.append(pd.DataFrame({\"Improved\":[improved],\"Worsened\":[worsened]}, index=[language]))\n",
    "    \n",
    "# 2) Chi-square comparing improvement rates across languages\n",
    "if len(all_tbls)>=2:\n",
    "    combo = pd.concat(all_tbls)\n",
    "    chi2, p, dof, exp = chi2_contingency(combo.values, correction=False)\n",
    "    print(\"\\\\n[FR vs ES] Chi-square on improvement rates\")\n",
    "    print(combo)\n",
    "    print(f\"Chi2={chi2:.3f}, dof={dof}, p={p:.4g}\")\n",
    "\n",
    "# 3) Improvement vs feature-change quartiles (per language, top features by |ridge_coef|)\n",
    "for language, d in all_results.items():\n",
    "    lp = d[\"line_paired\"]\n",
    "    if lp.empty:\n",
    "        continue\n",
    "    print(f\"\\\\n[{language.upper()}] Chi-square: Improvement vs Δ-feature quartiles\")\n",
    "    # pick top 3 by absolute ridge coef (from delta results if available)\n",
    "    dcols = [f\"d_{c}\" for c in FEATURES]\n",
    "    if language in results_delta:\n",
    "        top = results_delta[language][\"ridge_coefs\"].reindex(dcols).abs().sort_values(ascending=False).head(3).index.tolist()\n",
    "    else:\n",
    "        top = dcols[:3]\n",
    "    for col in top:\n",
    "        try:\n",
    "            q = pd.qcut(lp[col], 4, labels=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"])\n",
    "            tbl = pd.crosstab(q, (lp[\"d_wer\"]<0).map({True:\"Improved\",False:\"Worsened\"}))\n",
    "            chi2, p, dof, exp = chi2_contingency(tbl.values, correction=False)\n",
    "            print(f\" - {col}: Chi2={chi2:.3f}, dof={dof}, p={p:.4g}\")\n",
    "        except Exception as e:\n",
    "            print(f\" - {col}: skipped ({e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0556cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Mixed-effects confirmation on pooled long set ===\n",
    "mixed_results = {}\n",
    "for language, d in all_results.items():\n",
    "    pooled = d[\"pooled\"].copy()\n",
    "    if pooled.empty:\n",
    "        continue\n",
    "    # Standardize features within language (pooled)\n",
    "    for c in FEATURES:\n",
    "        pooled[f\"{c}_z\"] = (pooled[c] - pooled[c].mean()) / (pooled[c].std(ddof=0) + 1e-9)\n",
    "    # Build fixed effects: treatment + features + interactions (limited to top 3 to keep model stable)\n",
    "    # We'll pick top 3 Δ features from delta ridge for interactions guidance, otherwise first 3.\n",
    "    if language in results_delta:\n",
    "        top_base = [c.replace(\"d_\",\"\") for c in results_delta[language][\"ridge_coefs\"].abs().sort_values(ascending=False).head(3).index.tolist()]\n",
    "    else:\n",
    "        top_base = FEATURES[:3]\n",
    "    fixed_cols = [\"treatment\"] + [f\"{c}_z\" for c in FEATURES] + [f\"treatment:{c}_z\" for c in top_base]\n",
    "    \n",
    "    # Design matrices\n",
    "    exog = pd.DataFrame({\"Intercept\":1.0}, index=pooled.index)\n",
    "    for c in FEATURES:\n",
    "        exog[f\"{c}_z\"] = pooled[f\"{c}_z\"]\n",
    "    exog[\"treatment\"] = pooled[\"treatment\"].astype(int)\n",
    "    for c in top_base:\n",
    "        exog[f\"treatment:{c}_z\"] = exog[\"treatment\"] * exog[f\"{c}_z\"]\n",
    "    endog = pooled[\"log1p_wer\"]\n",
    "    \n",
    "    # Random intercepts: speaker, country, gender (use combined groups to approximate multiple REs)\n",
    "    # statsmodels MixedLM accepts one grouping var; we fit with speaker_id, then as a robustness fit with country-gender.\n",
    "    for grouping in [\"speaker_id\", None]:\n",
    "        if grouping is None:\n",
    "            pooled[\"cg\"] = pooled[\"country\"].astype(str) + \"|\" + pooled[\"gender\"].astype(str)\n",
    "            grouping = \"cg\"\n",
    "        try:\n",
    "            md = MixedLM(endog, exog, groups=pooled[grouping].astype(str))\n",
    "            mdf = md.fit(method=\"lbfgs\", reml=True, maxiter=200, disp=False)\n",
    "            mixed_results[(language, grouping)] = mdf\n",
    "            print(f\"[{language.upper()}] MixedLM fit with groups={grouping}: converged, AIC={mdf.aic:.1f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{language.upper()}] MixedLM failed with groups={grouping}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Simple mediation estimate (proportion explained) ===\n",
    "# We estimate how much of the treatment effect (theta) is explained by feature shifts.\n",
    "# Approach: compare two pooled OLS models — with and without features — and compute the\n",
    "# change in the estimated treatment coefficient. Bootstrap by speaker for CI.\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def treatment_coef(pooled, with_features=True, top_feats=None):\n",
    "    df = pooled.copy()\n",
    "    df[\"log1p_wer\"] = np.log1p(df[OUTCOME])\n",
    "    if with_features:\n",
    "        Xcols = [\"treatment\"] + top_feats\n",
    "    else:\n",
    "        Xcols = [\"treatment\"]\n",
    "    X = sm.add_constant(df[Xcols])\n",
    "    y = df[\"log1p_wer\"]\n",
    "    model = sm.OLS(y, X).fit(cov_type=\"cluster\", cov_kwds={\"groups\": df[\"speaker_id\"].astype(str)})\n",
    "    return model.params.get(\"treatment\", np.nan)\n",
    "\n",
    "def bootstrap_prop_mediated(pooled, B=200, top_feats=None):\n",
    "    speakers = pooled[\"speaker_id\"].astype(str).unique()\n",
    "    pm = []\n",
    "    for b in range(B):\n",
    "        samp = np.random.choice(speakers, size=len(speakers), replace=True)\n",
    "        dfb = pd.concat([pooled[pooled[\"speaker_id\"].astype(str)==s] for s in samp], ignore_index=True)\n",
    "        theta0 = treatment_coef(dfb, with_features=False, top_feats=top_feats)\n",
    "        theta1 = treatment_coef(dfb, with_features=True, top_feats=top_feats)\n",
    "        pm.append(1 - (theta1/theta0) if theta0!=0 else np.nan)\n",
    "    pm = pd.Series(pm).dropna()\n",
    "    return pm.mean(), pm.quantile([0.025, 0.975]).values\n",
    "\n",
    "for language, d in all_results.items():\n",
    "    pooled = d[\"pooled\"].copy()\n",
    "    if pooled.empty:\n",
    "        continue\n",
    "    # choose top features by |ridge| on deltas (fallback to all z-features if not available)\n",
    "    if language in results_delta:\n",
    "        top = [c.replace(\"d_\",\"\") for c in results_delta[language][\"ridge_coefs\"].abs().sort_values(ascending=False).head(4).index.tolist()]\n",
    "    else:\n",
    "        top = FEATURES\n",
    "    top_z = [f\"{c}_z\" for c in top]\n",
    "    # create z features\n",
    "    for c in FEATURES:\n",
    "        pooled[f\"{c}_z\"] = (pooled[c] - pooled[c].mean()) / (pooled[c].std(ddof=0) + 1e-9)\n",
    "    try:\n",
    "        mean_pm, (lo, hi) = bootstrap_prop_mediated(pooled, B=200, top_feats=top_z)\n",
    "        print(f\"[{language.upper()}] Estimated proportion of treatment effect mediated by features: {mean_pm:.2%} (95% CI {lo:.2%}–{hi:.2%})\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{language.upper()}] Mediation bootstrap failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick plots (optional) ===\n",
    "for language, d in all_results.items():\n",
    "    lp = d[\"line_paired\"]\n",
    "    if lp.empty:\n",
    "        continue\n",
    "    plt.figure()\n",
    "    plt.hist(lp[\"d_wer\"].dropna(), bins=40)\n",
    "    plt.title(f\"{language.capitalize()}: ΔWER histogram\")\n",
    "    plt.xlabel(\"ΔWER (transformed - original)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d80cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next steps you may customize\n",
    "- Adjust the **Config** cell paths/regex to match your data layout in Cursor.\n",
    "- If you want stricter causal language, keep analysis descriptive + mediation as *attribution* (not identification).\n",
    "- Consider exporting tables (coefficients, chi‑square results) to CSV for your report.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
